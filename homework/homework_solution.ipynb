{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Install Spark and PySpark**\n",
    "\n",
    "- Install Spark\n",
    "- Run PySpark\n",
    "- Create a local spark session\n",
    "- Execute spark.version.\n",
    "\n",
    "What's the output?\n",
    "\n",
    "- 3.3.2\n",
    "- 2.1.4\n",
    "- 1.2.3\n",
    "- 5.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answer is 3.3.3, as the version installed and run on my machine is 3.3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/10/30 08:16:44 WARN Utils: Your hostname, dawson-vm resolves to a loopback address: 127.0.1.1; using 192.168.192.134 instead (on interface ens33)\n",
      "23/10/30 08:16:44 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/10/30 08:16:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.Builder() \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.192.134:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f0a40a04400>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.3.3'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HVFHW June 2021**\n",
    "\n",
    "Read it with Spark using the same schema as we did in the lessons. \\\n",
    "We will use this dataset for all the remaining questions.\n",
    "\n",
    "Repartition it to 12 partitions and save it to parquet.\n",
    "\n",
    "What is the average size of the Parquet (ending with .parquet extension) Files that were created (in MB)? Select the answer which most closely matches.\n",
    "\n",
    "- 2MB\n",
    "- 24MB\n",
    "- 100MB\n",
    "- 250MB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answer is 24 MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_fhvhv = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"data/raw/fhvhv_tripdata_2021-06.csv.gz\", inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_fhvhv.repartition(12) \\\n",
    "    .write.parquet(\"data/pq/fhvhv/2021/06/\", mode=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 284M\n",
      "-rw-r--r-- 1 dawson dawson 24M Oct 30 08:37 part-00000-e9a57d8e-5ea5-46c2-9777-2043bc5f761a-c000.snappy.parquet\n",
      "-rw-r--r-- 1 dawson dawson 24M Oct 30 08:37 part-00001-e9a57d8e-5ea5-46c2-9777-2043bc5f761a-c000.snappy.parquet\n",
      "-rw-r--r-- 1 dawson dawson 24M Oct 30 08:37 part-00002-e9a57d8e-5ea5-46c2-9777-2043bc5f761a-c000.snappy.parquet\n",
      "-rw-r--r-- 1 dawson dawson 24M Oct 30 08:37 part-00003-e9a57d8e-5ea5-46c2-9777-2043bc5f761a-c000.snappy.parquet\n",
      "-rw-r--r-- 1 dawson dawson 24M Oct 30 08:37 part-00004-e9a57d8e-5ea5-46c2-9777-2043bc5f761a-c000.snappy.parquet\n",
      "-rw-r--r-- 1 dawson dawson 24M Oct 30 08:37 part-00005-e9a57d8e-5ea5-46c2-9777-2043bc5f761a-c000.snappy.parquet\n",
      "-rw-r--r-- 1 dawson dawson 24M Oct 30 08:37 part-00006-e9a57d8e-5ea5-46c2-9777-2043bc5f761a-c000.snappy.parquet\n",
      "-rw-r--r-- 1 dawson dawson 24M Oct 30 08:37 part-00007-e9a57d8e-5ea5-46c2-9777-2043bc5f761a-c000.snappy.parquet\n",
      "-rw-r--r-- 1 dawson dawson 24M Oct 30 08:37 part-00008-e9a57d8e-5ea5-46c2-9777-2043bc5f761a-c000.snappy.parquet\n",
      "-rw-r--r-- 1 dawson dawson 24M Oct 30 08:37 part-00009-e9a57d8e-5ea5-46c2-9777-2043bc5f761a-c000.snappy.parquet\n",
      "-rw-r--r-- 1 dawson dawson 24M Oct 30 08:37 part-00010-e9a57d8e-5ea5-46c2-9777-2043bc5f761a-c000.snappy.parquet\n",
      "-rw-r--r-- 1 dawson dawson 24M Oct 30 08:37 part-00011-e9a57d8e-5ea5-46c2-9777-2043bc5f761a-c000.snappy.parquet\n",
      "-rw-r--r-- 1 dawson dawson   0 Oct 30 08:37 _SUCCESS\n"
     ]
    }
   ],
   "source": [
    "! ls -lh './data/pq/fhvhv/2021/06'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count records\n",
    "\n",
    "How many taxi trips were there on June 15?\n",
    "\n",
    "Consider only trips that started on June 15.\n",
    "\n",
    "- 308,164\n",
    "- 12,856\n",
    "- 452,470\n",
    "- 50,982"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answer is 452470."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We will use pyspark.sql.functions as a helper function for this question and the next one.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[dispatching_base_num: string, pickup_datetime: timestamp, dropoff_datetime: timestamp, PULocationID: int, DOLocationID: int, SR_Flag: string, Affiliated_base_number: string]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fhvhv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "452470"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fhvhv.filter(F.to_date(df_fhvhv.pickup_datetime) == \"2021-06-15\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Longest trip for each day**\n",
    "\n",
    "Now calculate the duration for each trip.\n",
    "\n",
    "How long was the longest trip in Hours?\n",
    "\n",
    "- 66.87 Hours\n",
    "- 243.44 Hours\n",
    "- 7.68 Hours\n",
    "- 3.32 Hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer is 66.87 Hours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code below shows how to find answer in Pyspark Dataframe methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+--------+\n",
      "|   dropoff_datetime|    pickup_datetime|duration|\n",
      "+-------------------+-------------------+--------+\n",
      "|2021-06-28 08:48:25|2021-06-25 13:55:41|   66.88|\n",
      "+-------------------+-------------------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_fhvhv.select(\"dropoff_datetime\", \"pickup_datetime\") \\\n",
    "    .withColumn(\"duration\", F.round((F.unix_timestamp(df_fhvhv.dropoff_datetime) - F.unix_timestamp(df_fhvhv.pickup_datetime)) / 3600.0, 2)) \\\n",
    "    .orderBy(\"duration\", ascending=False) \\\n",
    "    .limit(1) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code below shows how to find answer in Spark SQL. \\\n",
    "It is necessary to register the dataframe to SQL as a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dawson/spark/spark-3.3.3-bin-hadoop3/python/pyspark/sql/dataframe.py:229: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "df_fhvhv.registerTempTable(\"fhvhv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "timediff_sql = spark.sql(\"\"\"\n",
    "SELECT  dropoff_datetime,\n",
    "        pickup_datetime,\n",
    "        ROUND((UNIX_TIMESTAMP(dropoff_datetime) - UNIX_TIMESTAMP(pickup_datetime)) / 3600.0, 2) AS duration\n",
    "FROM fhvhv\n",
    "ORDER BY 3 DESC\n",
    "LIMIT 1;                                                           \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+--------+\n",
      "|   dropoff_datetime|    pickup_datetime|duration|\n",
      "+-------------------+-------------------+--------+\n",
      "|2021-06-28 08:48:25|2021-06-25 13:55:41|   66.88|\n",
      "+-------------------+-------------------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "timediff_sql.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**User Interface**\n",
    "\n",
    "Spark’s User Interface which shows application's dashboard runs on which local port?\n",
    "\n",
    "- 80\n",
    "- 443\n",
    "- 4040\n",
    "- 8080\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answer is port 4040."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![interface](./pyspark-jobs-interface.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Most frequent pickup location zone**\n",
    "\n",
    "Load the zone lookup data into a temp view in Spark \\\n",
    "[Zone Data](https://github.com/DataTalksClub/nyc-tlc-data/releases/download/misc/taxi_zone_lookup.csv)\n",
    "\n",
    "Using the zone lookup data and the fhvhv June 2021 data, what is the name of the most frequent pickup location zone?\n",
    "\n",
    "- East Chelsea\n",
    "- Astoria\n",
    "- Union Sq\n",
    "- Crown Heights North"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer is Crown Heights North"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-10-30 09:59:24--  https://github.com/DataTalksClub/nyc-tlc-data/releases/download/misc/taxi_zone_lookup.csv\n",
      "Resolving github.com (github.com)... 20.205.243.166\n",
      "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/5a2cc2f5-b4cd-4584-9c62-a6ea97ed0e6a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20231030%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20231030T095849Z&X-Amz-Expires=300&X-Amz-Signature=e7f4549e7f56130de8f7cc6f205724acbe87630bd744e5a4135faac5bc8beef9&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=513814948&response-content-disposition=attachment%3B%20filename%3Dtaxi_zone_lookup.csv&response-content-type=application%2Foctet-stream [following]\n",
      "--2023-10-30 09:59:24--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/5a2cc2f5-b4cd-4584-9c62-a6ea97ed0e6a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20231030%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20231030T095849Z&X-Amz-Expires=300&X-Amz-Signature=e7f4549e7f56130de8f7cc6f205724acbe87630bd744e5a4135faac5bc8beef9&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=513814948&response-content-disposition=attachment%3B%20filename%3Dtaxi_zone_lookup.csv&response-content-type=application%2Foctet-stream\n",
      "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 12322 (12K) [application/octet-stream]\n",
      "Saving to: ‘taxi_zone_lookup.csv’\n",
      "\n",
      "taxi_zone_lookup.cs 100%[===================>]  12.03K  --.-KB/s    in 0s      \n",
      "\n",
      "2023-10-30 09:59:25 (67.1 MB/s) - ‘taxi_zone_lookup.csv’ saved [12322/12322]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/DataTalksClub/nyc-tlc-data/releases/download/misc/taxi_zone_lookup.csv -O taxi_zone_lookup.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "zone_lookup = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"taxi_zone_lookup.csv\", inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zone_lookup\n",
      "root\n",
      " |-- LocationID: integer (nullable = true)\n",
      " |-- Borough: string (nullable = true)\n",
      " |-- Zone: string (nullable = true)\n",
      " |-- service_zone: string (nullable = true)\n",
      "\n",
      "df_fhvhv\n",
      "root\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      " |-- Affiliated_base_number: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"zone_lookup\")\n",
    "zone_lookup.printSchema()\n",
    "\n",
    "print(\"df_fhvhv\")\n",
    "df_fhvhv.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section uses Spark Dataframe methods to find the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "zone_lookup_pu = zone_lookup.withColumnRenamed(\"Zone\", \"PUZone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 41:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+\n",
      "|             PUZone|count_pickup|\n",
      "+-------------------+------------+\n",
      "|Crown Heights North|      231279|\n",
      "+-------------------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_fhvhv.join(zone_lookup_pu, on=df_fhvhv.PULocationID == zone_lookup_pu.LocationID, how=\"inner\") \\\n",
    "    .groupBy(\"PUZone\") \\\n",
    "    .agg(F.count(\"PUZone\").alias(\"count_pickup\")) \\\n",
    "    .orderBy(\"count_pickup\", ascending=False) \\\n",
    "    .limit(1) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section uses Spark SQL to find the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "zone_lookup_pu.createOrReplaceTempView(\"zone_lookup_pu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 45:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+\n",
      "|             PUZone|count_pickup|\n",
      "+-------------------+------------+\n",
      "|Crown Heights North|      231279|\n",
      "+-------------------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "          SELECT z.PUZone, COUNT(1) AS count_pickup\n",
    "          FROM fhvhv f\n",
    "          INNER JOIN zone_lookup_pu z ON z.LocationID = f.PULocationID\n",
    "          GROUP BY z.PUZone\n",
    "          ORDER BY 2 DESC\n",
    "          LIMIT 1;\n",
    "          \"\"\") \\\n",
    "          .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "z2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
